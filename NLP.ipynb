{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yashgabani845/chatbot/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Basic NLP Concepts**"
      ],
      "metadata": {
        "id": "_lNY2NN0qdru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Preprocessing:\n"
      ],
      "metadata": {
        "id": "Ta9ddEV9qfc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenization in NLP**\n",
        "Tokenization is the process of breaking down text into smaller components\n",
        "like words or sentences. These components are called \"tokens\". Tokenization is one of the fundamental steps in natural language processing (NLP).\n",
        "\n",
        "\n",
        "\n",
        "Type of tokenization\n",
        "1.   Word\n",
        "2.   Sentence\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3G1Ykcbaqv6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wp6qEzB1rGW-",
        "outputId": "d2531452-a889-4e01-8042-6f0b220ec0a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n"
      ],
      "metadata": {
        "id": "sgDO7fEkrP3g"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')  # Download the tokenizer models\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "teststring = \"long-off long-off suryakumar yadav suryakumar yadav.... ne pakada hai apne careear ka sabse important catch \"\n",
        "\n",
        "tokens = word_tokenize(teststring)\n",
        "tokens"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XRzyOPWarT_o",
        "outputId": "778dfb69-4553-4115-b01d-575dfaf22b21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['long-off',\n",
              " 'long-off',\n",
              " 'suryakumar',\n",
              " 'yadav',\n",
              " 'suryakumar',\n",
              " 'yadav',\n",
              " '....',\n",
              " 'ne',\n",
              " 'pakada',\n",
              " 'hai',\n",
              " 'apne',\n",
              " 'careear',\n",
              " 'ka',\n",
              " 'sabse',\n",
              " 'important',\n",
              " 'catch']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentance Tokenization**"
      ],
      "metadata": {
        "id": "O5kVxHJLr5ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sentances = sent_tokenize(teststring)\n",
        "sentances"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rniDG-u2r95q",
        "outputId": "170e2fac-649d-40c6-e14c-b73eb3559b9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['long-off long-off suryakumar yadav suryakumar yadav.... ne pakada hai apne careear ka sabse important catch']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***With spacy***\n",
        "\n"
      ],
      "metadata": {
        "id": "fgmfHuH6sNDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "rkiWR_iVsTkS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(teststring)\n",
        "# Word Tokenization\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)\n",
        "sentences = [sent.text for sent in doc.sents]\n",
        "print(sentences)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "UJtBlRO9sXHx",
        "outputId": "5d48cc83-b81b-4270-8aa2-d4a5149d5378",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['long', '-', 'off', 'long', '-', 'off', 'suryakumar', 'yadav', 'suryakumar', 'yadav', '....', 'ne', 'pakada', 'hai', 'apne', 'careear', 'ka', 'sabse', 'important', 'catch']\n",
            "['long-off long-off suryakumar yadav suryakumar yadav....', 'ne pakada hai apne careear ka sabse important catch']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> For particular tokenization i would say nltk is quite better\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-Dc_d73Tsz7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparison Between NLTK and spaCy for Tokenization**\n",
        "\n",
        "\n",
        "Speed\n",
        "NLTK :Slower\t,Slightly more code required\t ,Rule-based and uses punkt model\t, Allows manual adjustments\n",
        "\n",
        "Spacy:\n",
        "Faster\n",
        "More intuitive API\n",
        "Based on rules and machine learning\n",
        "Highly customizable pipelines\n",
        "\n"
      ],
      "metadata": {
        "id": "zSqZTPcvtBEJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stemming and Lemmatization in NLP**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LX6Xn-lAuI3_"
      }
    }
  ]
}